\section{线性回归}

\subsection{预测方程}
\begin{equation}
    h_\theta(\boldsymbol{x}) = \sum_{i=0}^n \theta_ix_i
\end{equation}

其中规定 $x_0 = 1$。

若令
\begin{equation}
    \boldsymbol{\theta} = \begin{bmatrix}\theta_0 \\ \theta_1 \\ \vdots \\ \theta_n\end{bmatrix} \in \mathbb R^{n+1},
\boldsymbol{X} = \begin{bmatrix}x_0 \\ x_1 \\ \vdots \\ x_n\end{bmatrix} \in \mathbb R^{n+1}
\end{equation}

则预测方程为
\begin{equation}
    h_\theta(x) = \boldsymbol{\theta}^\mathrm T\boldsymbol{X}
\end{equation}

\subsection{代价函数}
线性回归的代价函数为
\begin{equation}
    J(\theta_j) = \dfrac{1}{2m}\sum_{i=1}^m(h_\theta(\boldsymbol{x}^{(i)}) - y^{(i)})^2
\end{equation}

此时梯度公式为
\begin{equation}
    \nabla J(\boldsymbol{\theta}) = \dfrac{1}{m}\sum_{i=1}^m(h_\theta(\boldsymbol{x}^{(i)}) - y^{(i)})\boldsymbol{x}^{(i)}_j
\end{equation}

\subsection{归一化}
1. 均值归一化为
\begin{equation}
    x_i \leftarrow \dfrac{x_i - \mu_i}{\max - \min}
\end{equation}

2. min-max 归一化为
\begin{equation}
    x_i \leftarrow \dfrac{x_i - \min}{\max - \min}
\end{equation}

\subsection{学习率}
1. 学习率太小时，收敛的速度可能会很慢；

2. 学习率太大时，可能会导致算法不收敛，代价函数 $J(\boldsymbol{\theta})$ 可能不会每次迭代都减小。

\subsection{正规方程法求 $\boldsymbol{\theta}$}

令
\begin{equation}
    X = \begin{bmatrix}1 & x^{(1)}_1 & x^{(1)}_2 & \cdots & x^{(1)}_n \\ 1 & x^{(2)}_1 & x^{(2)}_2 & \cdots & x^{(2)}_n 
        \\ 1 & \vdots & \vdots & \vdots & \vdots \\ 1 & x^{(m)}_1 & x^{(m)}_2 & \cdots & x^{(m)}_n\end{bmatrix} \in \mathbb R^{m \times (n+1)},
    y = \begin{bmatrix}y^{(1)} \\ y^{(2)} \\ \vdots \\ y^{(m)}\end{bmatrix} \in \mathbb R^m
\end{equation}

则
\begin{equation}
    \boldsymbol{\theta} = (X^\mathrm T X)^{-1}X^\mathrm Ty
\end{equation}

考虑到矩阵乘法的复杂度为 $\mathcal O(n^3)$，所以当 $n$ 不是很大时可以考虑使用正规方程法，否则使用梯度下降法。

$X^\mathrm T X$ 不可逆的情况有两种：
1. 某些特征之间存在线性关系，导致部分特征是冗余的；

2. 训练集中的样本数目小于特征数目。